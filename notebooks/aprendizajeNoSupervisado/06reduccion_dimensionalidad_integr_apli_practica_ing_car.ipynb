{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integración y Aplicación Práctica de la Ingeniería de Características\n",
    "\n",
    "En este capítulo, integraremos las técnicas y conceptos de ingeniería de características previamente discutidos, aplicándolos en un contexto práctico para resolver un problema real. Esta sección combinará la transformación, creación y selección de características, junto con la implementación de funciones personalizadas y la manipulación avanzada de datos.\n",
    "\n",
    "\n",
    "**Preparación de los Datos**\n",
    "\n",
    "La preparación de los datos es un paso fundamental en cualquier proyecto de machine learning. Involucra la limpieza, transformación y selección de las características más relevantes para mejorar el rendimiento del modelo. A continuación, se describe un flujo de trabajo típico para la preparación de datos en un entorno de machine learning.\n",
    "\n",
    "\n",
    "**Limpieza y Transformación de Datos**\n",
    "\n",
    "La limpieza de datos es esencial para eliminar o corregir valores faltantes, duplicados o incorrectos. La transformación de datos incluye la normalización, estandarización y aplicación de técnicas avanzadas de ingeniería de características, como las vistas en los capítulos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2  feature3  feature4  feature1^2  feature1 feature2  \\\n",
      "0 -0.323112 -1.599230  0.426196 -1.505121    0.104401           0.516731   \n",
      "1  1.623434  0.475167 -1.484683  0.137700    2.635538           0.771402   \n",
      "2  0.884509 -0.629113 -1.219270  0.169484    0.782357          -0.556456   \n",
      "3  0.434049  0.036822  1.304832  0.500994    0.188399           0.015983   \n",
      "4 -1.061365  1.404924  0.304251  0.804649    1.126495          -1.491136   \n",
      "\n",
      "   feature1 feature3  feature1 feature4  feature2^2  feature2 feature3  \\\n",
      "0          -0.137709           0.486323    2.557535          -0.681586   \n",
      "1          -2.410285           0.223546    0.225784          -0.705473   \n",
      "2          -1.078456           0.149910    0.395783           0.767059   \n",
      "3           0.566361           0.217456    0.001356           0.048047   \n",
      "4          -0.322921          -0.854026    1.973810           0.427449   \n",
      "\n",
      "   feature2 feature4  feature3^2  feature3 feature4  feature4^2  target  \n",
      "0           2.407034    0.181643          -0.641477    2.265390       0  \n",
      "1           0.065430    2.204285          -0.204440    0.018961       1  \n",
      "2          -0.106625    1.486620          -0.206647    0.028725       0  \n",
      "3           0.018448    1.702587           0.653714    0.250995       0  \n",
      "4           1.130470    0.092569           0.244815    0.647460       1  \n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar un conjunto de datos de ejemplo\n",
    "data = pd.read_csv('../../data/dataset.csv')\n",
    "\n",
    "# Limpiar datos: eliminar duplicados y manejar valores faltantes\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Separar la variable objetivo del resto de las características\n",
    "target = data['target']  # Extraemos la columna 'target'\n",
    "features = data.drop(columns=['target'])  # Excluimos 'target' de las características\n",
    "\n",
    "# Normalizar características numéricas\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Crear características polinómicas\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "features_poly = poly.fit_transform(features_scaled)\n",
    "\n",
    "# Agregar las características polinómicas al DataFrame original\n",
    "features_final = pd.DataFrame(features_poly, columns=poly.get_feature_names_out(features.columns))\n",
    "\n",
    "# Agregar nuevamente la columna 'target'\n",
    "data_final = pd.concat([features_final, target.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(data_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación y Selección de Características**\n",
    "\n",
    "La creación de nuevas características y la selección de las más relevantes es clave para optimizar el rendimiento del modelo. Usando técnicas avanzadas como operadores sobrecargados y funciones personalizadas, se pueden generar características que capturen relaciones complejas entre los datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características seleccionadas: Index(['feature2', 'feature4', 'feature2^2', 'feature2 feature4'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Crear nuevas características utilizando operadores sobrecargados\n",
    "data_final['Ratio_Feature1_Feature2'] = data_final['feature1'] / data_final['feature2']\n",
    "data_final['Product_Feature3_Feature4'] = data_final['feature3'] * data_final['feature4']\n",
    "\n",
    "# Seleccionar las características más importantes utilizando un modelo de bosque aleatorio\n",
    "X = data_final.drop('target', axis=1)\n",
    "y = data_final['target']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "important_features = X.columns[importances > np.percentile(importances, 75)]\n",
    "\n",
    "print(f\"Características seleccionadas: {important_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación de Funciones Personalizadas**\n",
    "\n",
    "Las funciones personalizadas permiten transformar los datos de manera específica para el dominio del problema. Estas funciones pueden escalar, normalizar o transformar las características de acuerdo con las necesidades particulares del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.376025\n",
      "1    0.963140\n",
      "2    0.740267\n",
      "3    0.604399\n",
      "4    0.153354\n",
      "Name: Feature_Scaled, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Definir una función personalizada para escalar datos entre 0 y 1\n",
    "def escalar_personalizado(x, min_val, max_val):\n",
    "    return (x - min_val) / (max_val - min_val)\n",
    "\n",
    "# Aplicar la función personalizada a una característica específica\n",
    "data_final['Feature_Scaled'] = data_final['feature1'].apply(escalar_personalizado, args=(data_final['feature1'].min(), data_final['feature1'].max()))\n",
    "\n",
    "print(data_final['Feature_Scaled'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento y Evaluación del Modelo**\n",
    "\n",
    "Finalmente, el conjunto de datos procesado se utiliza para entrenar un modelo de machine learning. La evaluación del modelo se realiza utilizando métricas como la precisión, la sensibilidad o el error cuadrático medio, dependiendo del tipo de problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[important_features], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de Random Forest\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precisión del modelo: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_ds02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
