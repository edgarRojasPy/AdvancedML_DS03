{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos y Técnicas Esenciales\n",
    "\n",
    "La ingeniería de características es un proceso crucial en el preprocesamiento de datos, que implica la creación, transformación y selección de características que mejoran el rendimiento de los modelos de machine learning. En este capítulo, exploraremos los conceptos fundamentales y las técnicas más comunes utilizadas en la ingeniería de características, así como su importancia en el desarrollo de modelos de aprendizaje automático más precisos y eficientes.\n",
    "\n",
    " \n",
    "\n",
    "### ¿Qué es la Ingeniería de Características?\n",
    "\n",
    "La ingeniería de características se refiere al proceso de usar el conocimiento del dominio para seleccionar y transformar las variables en un conjunto de datos, con el fin de mejorar la capacidad de un modelo de aprendizaje automático para hacer predicciones precisas. Este proceso es tanto un arte como una ciencia, ya que requiere una combinación de conocimiento profundo del dominio del problema y habilidades técnicas para manipular y transformar los datos.\n",
    "\n",
    "\n",
    "### Importancia de la Ingeniería de Características\n",
    "\n",
    "La calidad de las características en un conjunto de datos es uno de los factores más importantes que determinan el rendimiento de un modelo de machine learning. Un buen modelo con características mal diseñadas puede fallar en capturar las relaciones correctas en los datos, mientras que un modelo simple con características bien diseñadas puede superar a modelos más complejos. A continuación, se destacan algunas razones por las que la ingeniería de características es crucial:\n",
    "\n",
    "1. Mejora de la Precisión del Modelo: Características bien diseñadas permiten que los modelos aprendan patrones más relevantes en los datos, lo que mejora la precisión de las predicciones.\n",
    "2. Reducción de la Complejidad del Modelo: Al crear características que capturan información importante de manera más eficiente, se pueden utilizar modelos más simples que son más fáciles de interpretar y menos propensos a sobreajustarse.\n",
    "3. Aumento de la Interpretabilidad: Las características que representan directamente variables clave del problema son más fáciles de interpretar, lo que facilita la comprensión y explicación del modelo.\n",
    "4. Facilitación del Proceso de Modelado: La ingeniería de características puede ayudar a preparar los datos para modelos específicos, optimizando el proceso de entrenamiento y mejorando los resultados.\n",
    "\n",
    "\n",
    "Técnicas Comunes en la Ingeniería de Características\n",
    "\n",
    "Existen diversas técnicas utilizadas en la ingeniería de características, cada una adecuada para diferentes tipos de datos y problemas. A continuación, se describen algunas de las más comunes:\n",
    "\n",
    "1. Transformación de Características:\n",
    "- Normalización y Estandarización: La normalización ajusta los valores de las características a un rango específico (generalmente 0 a 1), mientras que la estandarización ajusta las características para que tengan media 0 y desviación estándar 1. Esto es particularmente importante cuando las características tienen diferentes unidades de medida.\n",
    "- Escalado Logarítmico: Se utiliza cuando las características tienen una distribución sesgada, lo que ayuda a reducir el impacto de valores extremos.\n",
    "- Discretización: Convierte características continuas en discretas, dividiendo el rango de valores en intervalos y asignando a cada intervalo una etiqueta.\n",
    "\n",
    "2. Creación de Características:\n",
    "- Interacción de Características: Crea nuevas características combinando dos o más variables existentes, lo que puede revelar relaciones no lineales que no son capturadas por las características individuales.\n",
    "- Operadores Sobrecargados: Utilizar operadores aritméticos o lógicos para combinar características y generar nuevas variables que capturan relaciones más complejas en los datos.\n",
    "- Funciones Personalizadas: Desarrollo de funciones específicas para transformar características de manera que capturen información importante del dominio del problema.\n",
    "\n",
    "3. Selección de Características:\n",
    "- Métodos de Filtrado: Selecciona características basadas en su relación con la variable objetivo, utilizando métricas como correlación, chi-cuadrado, o importancia de características.\n",
    "- Métodos de Wrapper: Evalúa subconjuntos de características utilizando un modelo predictivo, seleccionando el subconjunto que proporciona el mejor rendimiento.\n",
    "- Métodos Embebidos: Técnicas que seleccionan características durante el proceso de entrenamiento del modelo, como el Lasso o los árboles de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación Práctica de la Ingeniería de Características\n",
    "\n",
    "Para ilustrar cómo se implementan estas técnicas en la práctica, aplicaremos algunas de ellas a un conjunto de datos real, utilizando Python y scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cuadrático Medio: 0.4797\n",
      "Coeficiente de Determinación: 0.6345\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Cargar el dataset de California Housing\n",
    "# Este es un conjunto de datos con información sobre las casas en California, como el precio, el tamaño, etc.\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# X es la parte de los datos que tiene todas las características, como el tamaño de la casa, la cantidad de habitaciones, etc.\n",
    "# y es lo que queremos predecir, que en este caso es el precio de la casa.\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Características\n",
    "y = data.target  # Precio de las casas\n",
    "\n",
    "# Normalización y estandarización\n",
    "# Esto es como ajustar los números para que todos tengan el mismo \"tamaño\" y no se sientan tan diferentes entre sí.\n",
    "# Imagina que estamos jugando a un juego y necesitamos que todos empujen con la misma fuerza, sin que unos sean más fuertes que otros.\n",
    "scaler = StandardScaler()  # Esto prepara el juego de la normalización\n",
    "X_scaled = scaler.fit_transform(X)  # Cambiamos las características para que tengan la misma fuerza\n",
    "\n",
    "# Creación de características polinómicas\n",
    "# Aquí estamos creando nuevas características a partir de las viejas, como hacer nuevas combinaciones.\n",
    "# Es como cuando tienes dos piezas de Lego y las unes para crear algo nuevo.\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)  # Creamos las nuevas piezas con combinaciones de las viejas\n",
    "X_poly = poly.fit_transform(X_scaled)  # Unimos las piezas para crear nuevas características\n",
    "\n",
    "# Selección de características\n",
    "# Ahora, de todas las características que tenemos (las viejas y las nuevas), vamos a ver cuáles son las más importantes.\n",
    "# Usamos un modelo para calcular cuáles son las características que más afectan al precio de la casa.\n",
    "model = LinearRegression()  # Este es un modelo que ayuda a predecir el precio de la casa\n",
    "model.fit(X_poly, y)  # Le decimos al modelo que aprenda de los datos\n",
    "importance = np.abs(model.coef_)  # Obtenemos la importancia de cada característica, cuánto ayuda a predecir el precio\n",
    "\n",
    "# Filtrar características menos importantes\n",
    "# De todas las características, eliminamos las que no ayudan tanto.\n",
    "# Es como si tuvieras muchas piezas de Lego, pero decides dejar solo las que realmente sirven para construir algo genial.\n",
    "threshold = np.percentile(importance, 50)  # Definimos un umbral que divide las características buenas y malas\n",
    "X_selected = X_poly[:, importance > threshold]  # Conservamos solo las características importantes\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "# Ahora, vamos a separar los datos en dos partes:\n",
    "# Una parte la usamos para enseñar al modelo (entrenamiento), y la otra para probar si aprendió bien (prueba).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "# Le enseñamos al modelo cómo predecir el precio de las casas usando los datos de entrenamiento.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Usamos el modelo para hacer predicciones con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculamos el error del modelo, que nos dice cuánto nos equivocamos en promedio al predecir el precio.\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Imprimimos el resultado del error\n",
    "# El MSE (Error Cuadrático Medio) nos muestra cuán lejos estuvo nuestro modelo de los precios reales.\n",
    "print(f\"Error Cuadrático Medio: {mse:.4f}\")\n",
    "\n",
    "# Imprimimos el coeficiente de determinación (R-squared)\n",
    "# El coeficiente de determinación nos dice cuán bueno es nuestro modelo en predecir el precio de las casas.\n",
    "# El R-squared nos dice cuán bueno es nuestro modelo en predecir el precio de las casas.\n",
    "print(f\"Coeficiente de Determinación: {model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación sencilla de los pasos:\n",
    "1. Cargar los datos: Estamos utilizando información sobre casas en California, como el precio, el tamaño, y el número de habitaciones.\n",
    "\n",
    "2. Normalización: Hacemos que todas las características de las casas estén en la misma \"escala\". Imagina que todos están empujando una pelota con la misma fuerza, para que nadie sea más fuerte que el otro.\n",
    "\n",
    "3. Crear características polinómicas: A partir de las características originales (como el tamaño de la casa), creamos nuevas características combinando las viejas. Es como hacer una nueva figura con piezas de Lego.\n",
    "\n",
    "4. Seleccionar las mejores características: Después de crear nuevas características, analizamos cuáles son las más útiles para predecir el precio de las casas. Eliminamos las que no ayudan mucho, como si decidiéramos no usar piezas de Lego que no sirven.\n",
    "\n",
    "5. Dividir los datos: Separamos los datos en dos partes: una para enseñar al modelo y otra para ver si aprendió bien. Usamos una parte para \"entrenar\" al modelo y otra para \"probar\" lo que aprendió.\n",
    "\n",
    "6. Entrenar el modelo: Usamos los datos de entrenamiento para enseñar al modelo cómo predecir el precio de las casas.\n",
    "\n",
    "7. Evaluar el modelo: Finalmente, calculamos qué tan bien lo hizo el modelo comparando las predicciones con los precios reales de las casas. Cuanto más pequeño sea el error (MSE), mejor lo hizo el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_ds02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
